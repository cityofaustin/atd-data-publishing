#  enable request verification

if __name__ == '__main__' and __package__ is None:
    from os import sys, path
    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))

import pymssql
import pyodbc
import arrow
import requests
import json
import email_alert
from secrets import SOCRATA_CREDENTIALS
from secrets import ALERTS_DISTRIBUTION
from secrets import IDB_PROD_CREDENTIALS


import pdb


SOCRATA_SYNC_CORRIDORS = 'https://data.austintexas.gov/resource/efct-8fs9.json'
SOCRATA_RETIMINGS = 'https://data.austintexas.gov/resource/eyaq-uimn'

EMAIL_FOOTER = '''
    \n
    This is an automated message generated by Austin Transportation's Arterial Management Division. To unsubscribe, contact john.clary@austintexas.gov.
    '''
then = arrow.now()



def fetch_published_data(dataset_url):
    print('fetch published data')
    try:
        res = requests.get(dataset_url, verify=False)

    except requests.exceptions.HTTPError as e:
        raise e

    return res.json()



def connect_int_db(credentials):
    print('connecting to intersection database')

    conn = pyodbc.connect(
        'DRIVER={{SQL Server}};' 
            'SERVER={};'
            'PORT=1433;'
            'DATABASE={};'
            'UID={};'
            'PWD={}'
            .format(
                credentials['server'],
                credentials['database'],
                credentials['user'],
                credentials['password'] 
        ))

    cursor = conn.cursor()
    
    return conn


def get_int_db_data_as_dict(connection, table, key):
    print('get intersection database data')
    
    query = 'SELECT * FROM {}'.format(table)

    results = []

    grouped_data = {}

    cursor = connection.cursor()
    
    cursor.execute(query)

    columns = [column[0] for column in cursor.description]

    for row in cursor.fetchall():
        results.append(dict(zip(columns, row)))

    for row in results:
        new_key = str(int(row[key]))
        grouped_data[new_key] = row

    return grouped_data



def group_socrata_data(dataset, key):
    print('group socrata data')

    grouped_data = {}
    
    for row in dataset:
        new_key = str(row[key])
        grouped_data[new_key] = row

    return grouped_data




def detect_changes(new, old):
    print('detect changes')

    upsert = []  #  see https://dev.socrata.com/publishers/upsert.html
    delete = 0    

    for record in new:
        upsert.append(new[record])

    for record in old:
        lookup = old[record]['record_id']
        
        if lookup not in new:
            delete += 1

            upsert.append({ 
                'record_id': lookup,
                ':deleted': True
            })

    return { 
        'upsert': upsert,
        'delete': delete
    }


def upsert_open_data(payload, url):
    print('upsert open data ' + url)
    try:
        auth = (SOCRATA_CREDENTIALS['user'], SOCRATA_CREDENTIALS['password'])

        json_data = json.dumps(payload)

        res = requests.post(url, data=json_data, auth=auth, verify=False)

    except requests.exceptions.HTTPError as e:
        raise e
    
    return res.json()



def package_log_data(date, changes, response):
    
    timestamp = arrow.now().timestamp

    date = date.format('YYYY-MM-DD HH:mm:ss')
   
    if 'error' in response.keys():
        response_message = response['message']
        
        email_alert.send_email(ALERTS_DISTRIBUTION, 'DATA PROCESSING ALERT: Socrata Upload Status Update Failure', response_message + EMAIL_FOOTER)

        errors = ''
        updated = ''
        created = ''
        deleted = ''

    else:
        errors = response['Errors']
        updated = response['Rows Updated']
        created = response['Rows Created']
        deleted = response['Rows Deleted']
        response_message = ''

    no_update = changes['no_update']
    update_requests = changes['update']
    insert_requests = changes['insert']
    delete_requests = changes['delete']

    if changes['not_processed']:
        not_processed = str(changes['not_processed'])
    else:
        not_processed = ''
     
    return [ {
        'event': 'signal_status_update',
        'timestamp': timestamp, 
        'date_time':  date,
        'errors': errors ,
        'updated': updated,
        'created': created,
        'deleted': deleted,
        'no_update': no_update,
        'not_processed': not_processed,
        'response_message': response_message
    } ]

    

    
def main(date_time):
    print('starting stuff now')

    try:
       
        old_retiming_data = fetch_published_data(SOCRATA_RETIMINGS)

        old_sync_systems = fetch_published_data(SOCRATA_SYNC_CORRIDORS)

        conn = connect_int_db(IDB_PROD_CREDENTIALS)

        new_retiming_data = get_int_db_data_as_dict(conn, 'RETIMING_QUERY', 'ID')

        new_sync_systems = get_int_db_data_as_dict(conn, 'SYNC_INTERSECTIONS_QUERY', 'ID')

        retiming_detection_results = detect_changes(new_retiming_data, old_retiming_data)

        sync_systems_results = detect_changes(new_sync_systems, old_sync_systems)

        pdb.set_trace()

        return {
            'res': socrata_response,
            'res_historical': socrata_response_historical,
            'payload': socrata_payload[0],
            'logfile': logfile_data,
            'not_found': socrata_payload[1]
        }
    
    except Exception as e:
        print('Failed to process data for {}'.format(date_time))
        print(e)
        email_alert.send_email(ALERTS_DISTRIBUTION, 'DATA PROCESSING ALERT: Signal Retiming Update Failure', str(e) + EMAIL_FOOTER)
        raise e
 
results = main(then)

print(results['res'])
print('Elapsed time: {}'.format(str(arrow.now() - then)))
