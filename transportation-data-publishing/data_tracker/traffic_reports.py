"""
Parse COA traffic report feed and upload to postgREST database.

This feed is generated by some CTM black magic that extracts incident data from the public safety CAD databse and publishes it in the form of an RSS feed and cold fusion-powered HTML table.
See: http://www.ci.austin.tx.us/qact/qact_rss.cfm
"""
import os
import pdb
import traceback

import arrow
import feedparser
import requests

import _setpath
from config.secrets import *
from util import datautil
from util import emailutil
from util import jobutil
from util import logutil


def query(url, method, data=None, auth=JOB_DB_API_TOKEN):

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {auth}",
        "Prefer": "return=representation, resolution=merge-duplicates",
    }

    if method.upper() == "SELECT":
        res = requests.get(url, headers=headers)

    elif method.upper() == "UPSERT":
        res = requests.post(url, headers=headers, json=data)

    elif method.upper() == "DELETE":
        res = requests.delete(url, headers=headers)

    else:
        raise Exception("Unknown method requested.")

    res.raise_for_status()
    return res.json()


def eq_filter(url, field_name, field_val):
    return f"{url}?{field_name}=eq.{field_val}"


def in_filter(url, field_name, val_list):
    vals = ",".join(val_list)
    return f"{url}?{field_name}=in.({vals})"


def parse_feed(feed, config):
    """
    Extract feed data by applying some unfortunate hardcoded parsing logic to feed entries
    """
    records = []
    for entry in feed.entries:
        record = handle_record(entry, config)
        records.append(record)
    return records


def parse_title(title):
    #  Aassume feed will never have Euro sign (it is non-ascii)
    #  TODO: use regex
    title = title.replace("    ", "€")

    #  remove remaining whitespace clumps like so:
    title = " ".join(title.split())

    #  split into list on
    title = title.split("€")

    #  remove empty strings. reducing to two elements
    title = [elem for elem in title if elem]

    issue = title[1].replace("-", "").strip()
    address = title[0].replace("/", "&")

    return address, issue


def extract_geocode(summary):
    elements = summary.split("|")
    elements = [elem.strip() for elem in elements]
    return elements[1:3]


def handle_record(entry, config):
    #  turn rss feed entry into traffic report dict
    record = {}
    published_date = arrow.get(entry.published_parsed).replace(tzinfo="US/Central")
    status_date = arrow.now().format()

    #  compose record id from entry identifier (which is not wholly unique) and
    #  publication timestamp
    record_id = "{}_{}".format(entry.id, published_date.timestamp)
    record[config["primary_key"]] = record_id
    record[config["date_field"]] = published_date.format()
    record[config["status_field"]] = "ACTIVE"
    record[config["status_date_field"]] = status_date
    title = entry.title
    title = parse_title(title)
    record["address"] = title[0]
    record["issue_reported"] = title[1]
    geocode = extract_geocode(entry.summary)
    record["latitude"] = geocode[0]
    record["longitude"] = geocode[1]
    return record


def apply_status(records, field="traffic_report_status", status="ARCHIVED"):
    for record in records:
        record[field] = status
    return records


def timestamp(records, field="traffic_report_status_date_time"):
    for record in records:
        record[field] = arrow.now().format()
    return records


def main(config):

    active_records_endpoint = eq_filter(
        config["endpoint"], config["status_field"], "ACTIVE"
    )
    active_records = query(active_records_endpoint, "select")
    active_records_ids = [record[config["primary_key"]] for record in active_records]

    feed = feedparser.parse(config["feed_url"])
    feed_records = parse_feed(feed, config)
    feed_record_ids = [record[config["primary_key"]] for record in feed_records]

    new_records = [
        record
        for record in feed_records
        if record[config["primary_key"]] not in active_records_ids
    ]

    archive_records = [
        record
        for record in active_records
        if record[config["primary_key"]] not in feed_record_ids
    ]

    archive_records = apply_status(archive_records, field=config["status_field"])
    archive_records = timestamp(archive_records, field=config["status_date_field"])

    payload = new_records + archive_records

    if payload:
        res = query(config["endpoint"], "UPSERT", data=payload)

    return len(payload)


if __name__ == "__main__":
    script_name = os.path.basename(__file__).replace(".py", "")
    logfile = f"{LOG_DIRECTORY}/{script_name}.log"

    logger = logutil.timed_rotating_log(logfile)
    logger.info("START AT {}".format(arrow.now()))

    config = {
        "feed_url": "http://www.ci.austin.tx.us/qact/qact_rss.cfm",
        "endpoint": "http://transportation-data.austintexas.io/traffic_reports",
        "primary_key": "traffic_report_id",
        "status_field": "traffic_report_status",
        "date_field": "published_date",
        "status_date_field": "traffic_report_status_date_time",
    }

    try:
        job = jobutil.Job(
            name=script_name,
            url=JOB_DB_API_URL,
            source="rss",
            destination="postgrest",
            auth=JOB_DB_API_TOKEN,
        )

        job.start()

        results = main(config)

        job.result("success", records_processed=results)

        logger.info("END AT {}".format(arrow.now()))

    except Exception as e:
        error_text = traceback.format_exc()
        logger.error(error_text)

        emailutil.send_email(
            ALERTS_DISTRIBUTION,
            "Traffic Report Process Failure",
            str(error_text),
            EMAIL["user"],
            EMAIL["password"],
        )

        job.result("error", str(e))

        raise e
